{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mclinwong/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed is important, especially training the CNN+ReLU Net\n",
    "#Otherwise, the results will be very different\n",
    "myseed = 8974  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train&Test Data library\n",
    "matlib = '/Users/mclinwong/GitHub/CodesReproduction/DCN-DOA/Data/matlib'\n",
    "#Path to save figures\n",
    "figure_savepath = '/Users/mclinwong/GitHub/CodesReproduction/DCN-DOA/ReproducedCodes/Figures/'\n",
    "#Path to save the model\n",
    "pthpath = '/Users/mclinwong/GitHub/CodesReproduction/DCN-DOA/Data/pth/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "num_epoch = 600\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "# load model\n",
    "# autoencoder = torch.load('/Users/mclinwong/GitHub/CodesReproduction/DCN-DOA/ReproducedCodes/matlib/11_22/autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_est.shape: (19800, 2, 120)\n",
      "S_abs.shape: (19800, 240)\n",
      "S_label.shape: (19800, 120)\n",
      "S_label1.shape: (19800, 1, 120)\n",
      "Sample: 19800, L: 120, dim: 2\n"
     ]
    }
   ],
   "source": [
    "datapath = os.path.join(matlib, 'data2_trainlow.mat')\n",
    "read_data = scipy.io.loadmat(datapath)\n",
    "S_est = read_data['S_est']\n",
    "S_abs = read_data['S_abs']\n",
    "S_label = read_data['S_label']\n",
    "R_est = read_data['R_est']\n",
    "S_label1 = np.expand_dims(S_label, 2)\n",
    "[Sample, L, dim] = np.shape(S_est)\n",
    "S_est = S_est.transpose(0, 2, 1)\n",
    "S_label1 = S_label1.transpose(0, 2, 1)\n",
    "\n",
    "print(f'S_est.shape: {S_est.shape}')\n",
    "print(f'S_abs.shape: {S_abs.shape}')\n",
    "print(f'S_label.shape: {S_label.shape}')\n",
    "print(f'S_label1.shape: {S_label1.shape}')\n",
    "print(f'Sample: {Sample}, L: {L}, dim: {dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        data = self.data[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_est_train.shape: (15840, 2, 120), S_est_test.shape: (3960, 2, 120)\n",
      "S_abs_train.shape: (15840, 240), S_abs_test.shape: (3960, 240)\n",
      "S_label1_train.shape: (15840, 1, 120), S_label1_test.shape: (3960, 1, 120)\n"
     ]
    }
   ],
   "source": [
    "S_est_train, S_est_test, S_label1_train, S_label1_test = train_test_split(S_est, S_label1, test_size=0.2)\n",
    "S_abs_train, S_abs_test, S_label_train, S_label_test = train_test_split(S_abs, S_label, test_size=0.2)\n",
    "print(f'S_est_train.shape: {S_est_train.shape}, S_est_test.shape: {S_est_test.shape}')\n",
    "print(f'S_abs_train.shape: {S_abs_train.shape}, S_abs_test.shape: {S_abs_test.shape}')\n",
    "print(f'S_label1_train.shape: {S_label1_train.shape}, S_label1_test.shape: {S_label1_test.shape}')\n",
    "\n",
    "train_set = MakeDataset(S_est_train, S_label1_train)\n",
    "train_set_fcn = MakeDataset(S_abs_train, S_label_train)\n",
    "valid_set = MakeDataset(S_est_test, S_label1_test)\n",
    "valid_set_fcn = MakeDataset(S_abs_test, S_label_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "train_loader_fcn = DataLoader(train_set_fcn, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_fcn = DataLoader(valid_set_fcn, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of Conv1D: (batch_size, channels, seq_len)\n",
    "# length_out = (length_in - kernel_size + 2 * padding) / stride + 1\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,activ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(2,12,kernel_size=25, padding=12), nn.BatchNorm1d(12), activ())\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(12,6,kernel_size=15, padding=7),  nn.BatchNorm1d(6), activ())\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(6,3,kernel_size=5, padding=2), nn.BatchNorm1d(3), activ())\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(3,1,kernel_size=3, padding=1),nn.BatchNorm1d(1), activ())\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L=120, \n",
    "#in_num = 2*L = 240\n",
    "#out1_num = int(2*L/3) = 80\n",
    "#out2_num = int(4*L/9) = 53\n",
    "#out3_num = int(2*L/3) = 80\n",
    "#out4_num = L = 120\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, activa):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(2*L, int(2*L/3)), nn.BatchNorm1d(80), activa())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(int(2*L/3), int(4*L/9)), nn.BatchNorm1d(53), activa())\n",
    "        self.fc3 = nn.Sequential(nn.Linear(int(4*L/9), int(2*L/3)), nn.BatchNorm1d(80), activa())\n",
    "        self.fc4 = nn.Sequential(nn.Linear(int(2*L/3), L), nn.BatchNorm1d(120), activa())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, epoch, name):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for x, y in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to('cpu').float()\n",
    "            x = x.to(device)\n",
    "            y = y.to('cpu').float()\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_loss_list.append(train_loss)\n",
    "        print('Epoch: {}, Train Loss: {:.4f}'.format(epoch, train_loss))\n",
    "    \n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        \n",
    "        for x, y in tqdm(valid_loader):\n",
    "            x = x.to('cpu').float()\n",
    "            x = x.to(device)\n",
    "            y = y.to('cpu').float()\n",
    "            y = y.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            valid_loss.append(loss.item())\n",
    "            \n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        print('Epoch: {}, Valid Loss: {:.4f}'.format(epoch, valid_loss))\n",
    "    \n",
    "    #save loss as csv\n",
    "    id = np.arange(0, num_epoch)\n",
    "    datafarme = pd.DataFrame({'id':id ,'train_loss':train_loss_list, 'valid_loss':valid_loss_list})\n",
    "    datafarme.to_csv(figure_savepath + str(name) +'loss.csv', index=False, sep=',')\n",
    "    return train_loss_list, valid_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:04<00:00, 54.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.0596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 216.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Valid Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 63.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 321.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Valid Loss: 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 64.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 311.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Valid Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 62.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 294.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Valid Loss: 0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 65.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 279.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Valid Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 67.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 321.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Valid Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:04<00:00, 61.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 294.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Valid Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 67.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 319.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Valid Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 65.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 315.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Valid Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:04<00:00, 61.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 276.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Valid Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 63.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 334.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Valid Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:03<00:00, 65.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 248.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Valid Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/248 [00:00<00:02, 68.08it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "CNN_ReLU = CNN(nn.ReLU).to(device)\n",
    "CNN_Tanh = CNN(nn.Tanh).to(device)\n",
    "CNN_Sigmoid = CNN(nn.Sigmoid).to(device)\n",
    "\n",
    "DNN_ReLU = DNN(nn.ReLU).to(device)\n",
    "DNN_Tanh = DNN(nn.Tanh).to(device)\n",
    "DNN_Sigmoid = DNN(nn.Sigmoid).to(device)\n",
    "\n",
    "tll_cnn_relu, vll_cnn_relu = train(\n",
    "    CNN_ReLU, train_loader, valid_loader, num_epoch, 'CNN_ReLU')\n",
    "torch.save(CNN_ReLU, pthpath + 'cnnrelu.pth')\n",
    "\n",
    "tll_cnn_tanh, vll_cnn_tanh = train(\n",
    "    CNN_Tanh, train_loader, valid_loader, num_epoch, 'CNN_Tanh')\n",
    "torch.save(CNN_Tanh, pthpath + 'cnntanh.pth')\n",
    "\n",
    "tll_cnn_sigmoid, vll_cnn_sigmoid = train(\n",
    "    CNN_Sigmoid, train_loader, valid_loader, num_epoch, 'CNN_Sigmoid')\n",
    "torch.save(CNN_Sigmoid, pthpath + 'cnnsigmoid.pth')\n",
    " \n",
    "tll_dnn_relu, vll_dnn_relu = train(\n",
    "    DNN_ReLU, train_loader_fcn, valid_loader_fcn, num_epoch, 'DNN_ReLU')\n",
    "torch.save(DNN_ReLU, pthpath + 'dnnrelu.pth')\n",
    "\n",
    "tll_dnn_tanh, vll_dnn_tanh = train(\n",
    "    DNN_Tanh, train_loader_fcn, valid_loader_fcn, num_epoch, 'DNN_Tanh')\n",
    "torch.save(DNN_Tanh, pthpath + 'dnntanh.pth')\n",
    "\n",
    "tll_dnn_sigmoid, vll_dnn_sigmoid = train(\n",
    "    DNN_Sigmoid, train_loader_fcn, valid_loader_fcn, num_epoch, 'DNN_Sigmoid')\n",
    "torch.save(DNN_Sigmoid, pthpath + 'dnnsigmoid.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save ,Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tll_cnn_relu_mult1000 = [i*1000 for i in tll_cnn_relu]\n",
    "vll_cnn_relu_mult1000 = [i*1000 for i in vll_cnn_relu]\n",
    "tll_cnn_tanh_mult1000 = [i*1000 for i in tll_cnn_tanh]\n",
    "vll_cnn_tanh_mult1000 = [i*1000 for i in vll_cnn_tanh]\n",
    "tll_dnn_relu_mult1000 = [i*1000 for i in tll_dnn_relu]\n",
    "vll_dnn_relu_mult1000 = [i*1000 for i in vll_dnn_relu]\n",
    "tll_dnn_tanh_mult1000 = [i*1000 for i in tll_dnn_tanh]\n",
    "vll_dnn_tanh_mult1000 = [i*1000 for i in vll_dnn_tanh]\n",
    "\n",
    "# save as csv\n",
    "import pandas as pd\n",
    "epcs = np.arange(0, num_epoch)\n",
    "df = pd.DataFrame({'epcho':epcs, 'tll_cnn_relu_mult1000':tll_cnn_relu_mult1000, 'vll_cnn_relu_mult1000':vll_cnn_relu_mult1000, 'tll_cnn_tanh_mult1000':tll_cnn_tanh_mult1000, 'vll_cnn_tanh_mult1000':vll_cnn_tanh_mult1000, 'tll_dnn_relu_mult1000':tll_dnn_relu_mult1000, 'vll_dnn_relu_mult1000':vll_dnn_relu_mult1000, 'tll_dnn_tanh_mult1000':tll_dnn_tanh_mult1000, 'vll_dnn_tanh_mult1000':vll_dnn_tanh_mult1000})\n",
    "df.to_csv(figure_savepath + 'loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(['science']):\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.ylim(0, 20)\n",
    "    plt.plot(tll_cnn_relu_mult1000, label='DCN relu')\n",
    "    plt.plot(tll_cnn_tanh_mult1000, label='DCN Tanh')\n",
    "    plt.plot(tll_dnn_relu_mult1000, label='DNN relu')\n",
    "    plt.plot(tll_dnn_tanh_mult1000, label='DNN Tanh')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train MSE ($ *10^{-3}$)')\n",
    "    plt.legend()\n",
    "    #save figure\n",
    "    plt.savefig(figure_savepath + 'Train Loss_compare.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(['science']):\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.ylim(0, 20)\n",
    "    plt.plot(valid_loss_list_cnn_ReLu_mult1000, label='DCN ReLU')\n",
    "    plt.plot(valid_loss_list_cnn_tanh_mult1000, label='DCN Tanh')\n",
    "    plt.plot(valid_loss_list_dnn_ReLu_mult1000, label='DNN ReLu')\n",
    "    plt.plot(valid_loss_list_dnn_Tanh_mult1000, label='DNN Tanh')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Valid MSE ($ *10^{-3}$)')\n",
    "    plt.legend()\n",
    "    #save figure\n",
    "    plt.savefig(figure_savepath + 'Valid Loss_compare.pdf')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b2b9bf415f84b889e420ad7a8860db48801ef28c10f7deabf8a2d1409fae0b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
